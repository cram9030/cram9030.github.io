---
layout: default
title: "Is Asking an LLM Worth The Time?"
date: 2025-01-19
---

Like everyone else, over the past few years, I've been playing around with some of the more prolific Large Language Models(LLM), trying to work them into a general workflow to figure out what works the best for me. Some of it is figuring out useful prompts, but a lot of it is figuring out what other existing tools are just faster, given my current knowledge and skill level. I remember the XKCD comic below of the "Is It Worth The Time?" chart, which inspired me to try and set up a running comparison of things that were not worth the time.

A quick disclaimer: I don't pretend that these observations are universal, and I try to note when I think poor prompting or my lack of knowledge is the likely root cause of a poor result.

<div style="text-align: center;">
    <a href="https://xkcd.com/1205/" title="Is It Worth the Time?">
        <img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png" alt="XKCD Time Management Chart">
    </a>
    <p style="font-size: 0.9em; margin-top: 8px;">
        <a href="https://xkcd.com/1205/">Is It Worth the Time?</a> by 
        <a href="https://xkcd.com">XKCD</a>, licensed under 
        <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC BY-NC 2.5</a>
    </p>
</div>

Most of the issues I’ve observed fall into a few different categories, listed in no particular order.

1. Exploratory question turns into an implementation request, which provides an overly complicated solution
2. Too large of a chunk of work requested
3. Troubleshooting results in excessively complicated implementations
4. API (Application Programming Interface) related questions

In many ways, the LLMs having issues with these sorts of requests makes a lot of sense. The first one centers around not having enough initial information to articulate what we want to do in detail, which is essentially a task that lacks clear existing requirements. This is one of the hardest things because coming up with what we want is much harder than saying what we don’t want. This has spawned numerous humorous cartoons, YouTube videos, and fully fledged industries.

The second issue comes down to another common problem: breaking down work into its smallest chunks from which to start executing. The larger the scope, the harder it is to find a starting point. The combination of the first and second problems are some of the primary motivating factors for Agile Development, where accepting that we don’t have a good idea of the requirements means we should create something to help determine the actual needs as quickly as possible and focus on short time and task decomposition forces smaller tasks and better execution.

The third and fourth issues would seem to stem not from a fundamental complication with development itself but from the source training material. While there is a lot of content online with simple examples, it is usually through the examples of how to use the library provided. If those examples were working and sufficient, though, there would not really need to be any troubleshooting. More often, we start deviating from those examples, and when we start troubleshooting, that might warrant a more complicated solution because the simple one was already disposed of by the initial documentation. Some prominent exceptions are “it works on my machine” or environment setup issues, linking issues, or concepts that are so “fundamental” that they don’t get covered in examples. Though it is probably worth acknowledging that some of this often falls into the unclear requirement camp where a more complicated solution might be better but the time it takes to implement matters as well and is frequently an unwritten, unquantified requirement.

The APIs are challenging well because the reasons they are challenging are pretty unique and often have single sources of truth that work off a combination of inputs. An example of this is when I tried to use MoveIt in a Python development environment. A quick googling made me think that [moveit_py](https://github.com/moveit/moveit2/blob/main/moveit_py/README.md) seemed like a greate choice there were some examples that I wanted to adapt so I asked Claude to adapt them but it never really worked. It kept mixing API called between different versions of MoveIt that weren't the current version I was trying to use. Then after all of it I was able to determine the core issue was [ROS Humble was just not going to support those features](https://moveit.ai/moveit%202/ros/2023/05/31/balancing-stability-and-development.html). So it was an API problem where it assumed what the API should be based on the API through the rest of the library instead of *knowing* what the API actually was. On top of that there was that pesky version, environment, and support issue which funnily enough did warrent the more complicated solution though the real answer was to just use the API that called the C++ version of the code that was supported in the environment. 

In any case I'm going to keep a running table of things that did actually save me time and I might update the prose above as I develop and learn more. All the contents of this table was done with Claude and I have not attempted to recreate it with any of the other prominant LLMs.

| Task | Result | Time Saved? | Commentary |
|------------------|--------|-------|------------|
| Making a this personal website | <span class="cell-yellow">⚠️ Mixed</span> | <span class="cell-yellow">Little</span> |The fact that this was not really a net time saver was a little surprising. It started out well with the initial suggestion to use GitHub Pages, and Jekyll was good, and the first website suggestions worked pretty well. The time that was gained with the initial spin-up was pretty quickly lost when it started suggesting customized GitHub actions that the core functionality already existed through the Pages deployment. I ended up having to go read the GitHub pages and Jekyll documentation anyway to be able to correct my issues. <br><br> I spent probably a solid afternoon, maybe 4 or 5 hrs playing around and getting things working. I was starting from zero, and I thought with Claude, I would half that time. I probably would have taken roughly the same amount of time if I had just spent the first hour reviewing the examples on GitHub pages.|
| Making this table| <span class="cell-green">✅ Good</span> | <span class="cell-green">Realtively Lage</span> | A single prompt got this working, and a quick googling suggests I would have probably struggled to try to make this work in markdown before realizing I really needed to update the CSS. It wasn't a lot of absolute time, but relative to the task, it was a significant proportion.|
| Embedding React Visualization| <span class="cell-green">✅ Good</span> |  <span class="cell-green">Realtively Lage</span> | In my first blog post I was showing my quick visualization that I had created just to help get some face validity on what I wanted to do. I woudn't have really known where to start but the solution worked off the first prompt and the only follow ups were asking for it in a seperate artifact.|
| Resolving Import Errors | <span class="cell-yellow">⚠️ Nuetral </span> | <span class="cell-yellow">No Gain</span> | Honestly, just writing out the problem to put in the prompt had a bigger impact on my solving the issue than the response did.|
| Docker Copy/Pathing Issues | <span class="cell-red">❌ Bad </span> | <span class="cell-red">Time Lost </span> | I've tried this a few times in a few different ways. This is one of the more frustrating things to deal with when trying to put together your project structure and test environments. There are a lot of best practices that are unique to the goal. The hope would be that using an LLM could help transfer those over, but it's been pretty bad. I have always had to end up going back to the source documentation and other posts to actually solve my issues. More often than not, I just end up using Claude to create a consistent docker-compose or dockerfile after figuring out the best way to do what I want. Completely removing the initial proposal from my first prompt.|
| Binary Lookup Table Storage for DLLs| <span class="cell-green">✅ Good</span> | <span class="cell-yellow">No Gain</span> |There wasn't really a time gain here because I would have implemented a solution just as fast without the feedback but the solution proposed was a good one (basically what I had been planning to do just simply static array ```static constexpr std::array<std::pair<float,float>, 7>```) so if I hadn't already known what I wanted it would have saved time for maybe a student or an rusty developer looking for confirmation.|
| Almost anything with ROS2| <span class="cell-red">❌ Bad </span> | <span class="cell-red">Time Lost </span> | I should start by saying I've used ROS2 a few times before. I'm not what I would consider an expert, but a functional user. Ideally, that seemed like the level at which I could give good enough prompts that were contained enough to get effective answers to be used when developing. That is not how it turned out. I spent much more time reading, troubleshooting, checking, and trying to understand the proposed solutions. A good chunk of this could have been user error, but they were always over-complicated solutions that didn't really leverage ROS2 as a development platform, almost always opting to create things from scratch when there were existing messages that could achieve the objectives, for example. I don't know when I will try this again, but I am pretty sure just starting from an existing ROS2 example, and editing would have saved me a significant amount of time instead of asking Claude to start from the same example and update it to my needs.<br><br>The one advantage that I noted was that it put in a lot more good quality checks into my code than I would have put in naturally at this stage of development. They were almost always correct and probably would have been things I went back to do once the core logic was working, but they got implemented earlier with little to no additional effort on my part.|
|Simplifying Existing ROS2 Node| <span class="cell-green">✅ Good</span> |  <span class="cell-green">Meaningful Gain</span> |The one exception to the row above was when I already had a node that I wanted to keep the interface the same and simply update the core business logic. In that case I provided the drawings, the equations I wanted implemented, and the existing code with the features I needed and it was able to pump out something that functioned as intended and with minor followups get it executing in the framework I wanted.|
|Writting git commit messages| <span class="cell-green">✅ Good</span> | <span class="cell-yellow">No Gain</span> |The one exception to the row above was when I already had a node for which I wanted to keep the interface the same and simply update the core business logic. In that case, I provided the drawings, the equations I wanted implemented, and the existing code with the features I needed, and it was able to pump out something that functioned as intended and, with minor follow-ups, execute in the framework I wanted.|
|Implementing Complex Equations| <span class="cell-red">❌ Bad </span> | <span class="cell-red">Time Lost </span> |I had thought with the simplifying existing ROS2 example above, I had cracked the code and that as long as I wrote out the math (something I almost always start with anyway before I go to code) that, I'd be able to crank out full classes just by copying and pasting. After all, all it have to do is move either Latex or the print output of SymPy into a function. Well, it really did not want to when I gave it a big, long equation with a bunch of states and decimals. Two of the equations could have one of the constants pulled out and multiplied after the fact without being passed into the function itself, but the other four couldn't. Instead of just passing the values into the functions for the other four equations what it instead opted to do was just remove the parts of the equations prevented that from happening so it could be consistent between all the functions format! They were pretty long nonlinear equations with something around 30 unique coefficient/variable combinations, so having to go line by line to confirm what was and wasn't lost was incredibly tedious.<br><br>This is a case where what I should have done is just let it write the interface and populated the equations myself. I already had a version I could copy and paste from the output of SymPy, which I would just need to match the parameters.|